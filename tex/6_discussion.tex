% !TEX root = ../thesis.tex
% !TEX spellcheck = en-US

\clearpage
\section{Discussion and Conclusions}

\subsection{Discussion of Experimental Results}

\subsection{Conclusions}
\label{subs:conclusions}

\begin{itemize}
  \item As in many areas of machine learning much work has been going into feature engineering but it seems that feature learning, while much more computationally expensive, surpasses the potential of engineered feature representations. Deep learning and meta-learning are mature enough to make up for the gap that has been there for years: To achieve performance that is good enough to make an algorithmic system usable in production, huge amounts of research and engineering went into feature engineering and finally the performance of these methods can be matched and even surpassed by automated methods or learning features. (link here NG's transfer learning work, also Schmidhubers work of meta-learning and on function prediction etc)
  \item There is more need to understand the representations of such feature learning systems though, statistics are quite easy to understand but weights of a neural network don't tell much. There is however potential for learning ``better statistics'' ourselves, e.g. how to efficiently learn a language (by looking at explicit indermediate representations of the states of a NN)
  \item
\end{itemize}


\subsection{Contributions}
\label{sub:contributions}

\begin{itemize}
  \item compare n-gram and doc2vec (?)
  \item
\end{itemize}

\subsection{Proposal for Future Research}
\label{sub:further-research}

\begin{itemize}
  \item how well do word2vec and comparable methods generalize: e.g.\ initialize a text corpus with word vectors from a bigger corpus (Google News), then train an RNN to predict the next word vector using the small corpus but use the bigger corpus to validate and see if words in bigger corpus can be inferred
  \item trajectory based algorithms (word trajectory through space for a sentence)
  \item Compare with standard benchmarks (TREC etc)
  \item Meta- / Transfer-learning: OCR with simultaneous LM learning (e.g. predict next character)
\end{itemize}

\paragraph{Trajectory -Based Algorithms on Text}
\label{par:Trajectory Algorithms on Text}
As shown in \cite{Mikolov:2013ac} and related work that was outlined in Section\todo{reference here} vector space models for text can capture very subtle semantic relationships between words by their location in the vector space. When


\subsection{Learnings}
\label{sub:learnings}

\begin{itemize}
  \item focusing on both, building a working system (engineering) and exploring new directions (science), is hard
  \item problem framing is hard
\end{itemize}
