% !TEX root = ../thesis.tex
% !TEX spellcheck = en-US

\clearpage

\section{Discussion}
\label{sec:Discussion}


\subsection{Approach 1: Vector Space Models}
\label{sub:Approach 1: Vector Space Models (Results)}

\subsubsection{N-gram Models}

Not without reason are N-Gram Models still so widely used as these experiments confirm. They are easy to understand, relatively straightforward to train, fast to compute and yield good performance for many problems in \gls{NLP}.

% on stop words
It seems there is information carried within these stop words. Of course this outcome is also influenced by the particular stop-list used (see Section~\ref{subp:Stop words}).

% on size
Thus we can conclude that size matters and bigger is better as higher-dimensional vectors can capture more information about N-gram occurrences. However performance gain with increased dimensionality diminishes beyond a certain threshold which depends on the dataset and further models start to overfit, harming generalization performance.
Also in practice the vector size must always be limited as it grows with the vocabulary, potentially at an exponential rate if N-grams larger than Unigrams are used.

% IDF
 Thus it seems advisable to leave this parameter free for and evaluate both variants with a given classifier. For logistic regression however the performance differences are marginal and so the choice for this parameter seems somewhat arbitrary.

 % # normalizing

 Is seems that normalizing the vectors in most cases does not lead to any performance gains. Again this is an often recommended practice but here it does not seem to add any value to the model.


\subsubsection{Bag-of-Means Model}

%performs well!
This is surprising as performance previously reported to be rather poor as mention in Section~\ref{subp:Bag-of-Means}.

 Further investigation into the use of different classifiers could shed light into these diverging results which are not observed using the N-gram models in the section above.

\subsubsection{Paragraph Vectors using Distributed Representations}

% on hyper parameters
and which turn out to have a huge influence on its performance as the results below indicate

% vector size
- even in 2 dimensions very expressive
% frequenct workd subsampling
- problably doesn't work because dataset small anyway
- when using PM-DV this significantly improved training score but test score was still poor
% hs
This result is counter-intuitive as using the hierarchical softmax should as an approximation be less performant. However it might simply mitigate overfitting of the model.
% window size
It is safe to assume that increasing the window size much further does not lead to any improvement in the model as the correlation with the word should become weaker the farther we move away from it in a document or text.
% Model type dbow better
This is in contrast with the results in the aforementioned paper, where the authors state that \textquote{PV-DM is consistently better than PV-DBOW.}~\cite{Le:2014aa}.

- tend to overfit easily



\todo{show computation time?}

\paragraph{Evaluating the best hyper-parameter setting}

Taking the learnings about the effects of the different hyper-parameters to the performance of the model a subset of models were tested in search of the best hyper-parameter selection. The results of these experiments can be seen in Table~\ref{tab:Paragraph Vector Parameter Results Best}.

A few interesting observations can be made here. First, as indicated before, the model is highly sensitive to the settings of the hyper-parameters. Secondly we can see that the hyper-parameters interact quite strongly in some combinations. This leads to a different behavior in performance for some of the hyper-parameters than identified in the above sections, depending on what the other hyper-parameters settings are.

For instance using hierarchical softmax decreases performance when setting all other parameters to individually optimal settings, as opposed to in the previous experiment above.
\todo{write a bit more here}


% \subsubsection{Paragraph Vectors using pre-initialized weights *}
%
% In another experiment the weight matrix for the words was initialized with pre-trained weights from from the Google News dataset.

% \subsubsection{Paragraph Vectors using context sentences *}
%
% \todo{This section in further research? Because it would have to be done for N-grams as well (building a model with the context around a sentence) and it doesn't fit the task (only sentence given). Could also go into exploration}

% \subsubsection{Inversion of Distributed Language Representations (??)}
%
% \todo{if this is described here it has to go into background as well}

- A note about train/validation/test splits (by testing the vector space models on the test data I leaked information on the test set into the training/model selection)


\subsection{Approach 2: Sequential Models}
\label{sub:Approach 2: Sequential Models (Results)}



\subsection{Comparison of Approaches}
