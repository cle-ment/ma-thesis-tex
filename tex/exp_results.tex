% !TEX root = ../thesis.tex
% !TEX spellcheck = en-US

\thispagestyle{empty}

\section{Experiments and Results}
\label{sec:Experiments and Results}

With the final problem definition and dataset in place a series of experiments were conducted to evaluate the performance of the different approaches explained in Section~\ref{sec:Methods}.
This part of the thesis lays out these experiments and their results. First the research objectives will be reiterated. Next the experimental setup for each of the methods will be described and the outcomes and observations will be presented.

\subsection{Objectives}
\label{sub:Objectives}

The experiments had simple objectives: The goal was to evaluate each method in terms of its effectiveness using a prediction metric well-suited for this problem. The metric used was \gls{MCC} as described in Section~\ref{sub:Performance Metric: Matthews Correlation Coefficient}. Effectiveness in particular meant comparing the algorithms with regards to their overall performance on test data and their ability to generalize well, i.e. to be robust against overfitting.
% Further considerations were taken into account such as the simplicity of the model in terms of model assumptions but also ease of use and interpretability, as well as algorithmic time and space complexity requirements.


\subsection{Baseline}
\label{sub:Baseline}

In order to have a reference point for predictive performance that any method should surpass two guessing strategies were used, namely uniform and stratified guessing. Uniform guessing means sampling from a uniform distribution, commonly known as ``rolling dice'', while stratified guessing refers to an estimator that samples a label for a data point using the observed distribution of labels in the data. Both methods effectively ignore the data itself when producing labels and as such any predictive algorithm should do better.

Averaged over 1000 runs both strategies yielded an \gls{MCC} score close to zero. Accuracy by comparison was around 0.16 for uniform guessing and 0.26 for stratified guessing. These results are within our expectations and further highlight the rationale for choosing \gls{MCC} as the principal metric: \gls{MCC} is zero for uninformed predictions for either strategy whereas the accuracy in the uniform setting corresponds simply to a value of $1/k$ where $k = 6$ is the number of labels. The accuracy in the stratified setting reveals improves by taking advantage of knowledge about the skew in the label distribution. Figure~\ref{fig:guessing-conf-matrix} shows the confusion matrices for these baseline variants in absolute and normalized form, where the described properties these guessing strategies can be observed.

\begin{figure}
 % From http://localhost:8888/notebooks/thesis/experiments/vector-space-models/Vector%20Space%20Models.ipynb#Baseline:-Guessing-Strategies
    \centering
    \begin{subfigure}[b]{0.47\textwidth}
        \includegraphics[width=\textwidth]{img/exp-vector-space/guessing-conf-matrix-uniform.pdf}
        \caption{Uniform guessing}
\label{fig:guessing-conf-matrix-uniform}
    \end{subfigure}
~\begin{subfigure}[b]{0.48\textwidth}
        \includegraphics[width=\textwidth]{img/exp-vector-space/guessing-conf-matrix-uniform-normalized.pdf}
        \caption{Uniform guessing (normalized)}
\label{fig:guessing-conf-matrix-uniform-normalized}
    \end{subfigure}
~\begin{subfigure}[b]{0.47\textwidth}
        \includegraphics[width=\textwidth]{img/exp-vector-space/guessing-conf-matrix-stratified.pdf}
        \caption{Stratified guessing}
\label{fig:guessing-conf-matrix-stratified}
    \end{subfigure}
~\begin{subfigure}[b]{0.48\textwidth}
        \includegraphics[width=\textwidth]{img/exp-vector-space/guessing-conf-matrix-stratified-normalized.pdf}
        \caption{Stratified guessing (normalized)}
\label{fig:guessing-stratified-normalized}
    \end{subfigure}
    \caption{Confusion matrices of uniform and stratified guessing strategies.}
\label{fig:guessing-conf-matrix}
\end{figure}

\clearpage

\subsection{Classification With Vector Space Models}
\label{sub:Classification With Vector Space Models}

A popular way to approach text classification and other tasks in natural language processing is to build a model that maps data into a vector space. Distance between data points in this space then translates to similarity between the objects (see Section~\ref{sub:Vector Space Models}).
The resulting vector representation of the data can then be fed into various learning algorithms. This section describes the experiments performed to evaluate such approaches.

First the different methods to produce such vector spaces are compared. Several methods were used to generate vectors from the data while limiting dimensionality to 300 for comparability --- a heuristically chosen value as performance for the models did not increase significantly beyond it. These vector representations were then compared in terms of performance by using them as input to the simple classification model Logistic Regression.
Second, the best performing configurations for each type of method were chosen and a set of various classification techniques were applied to them which were described previously in Section~\ref{sub:Methods For Classification With Vector Space Models}.

\subsubsection{N-gram Models}
\label{subs:N-gram Models (Experiments and Results)}

The first class of language models that was investigated for the task of multi-class classification are N-gram models that were explained in Section~\ref{subs:N-gram Models (Methods)}.
N-gram models come in a variety of forms. In these experiments the most common variants were set up as hyper-parameters to the model as  listed in Table~\ref{tab:N-gram Hyper-parameters Space}.

\begin{table}[h]
  \begin{center}
  \begin{tabular}{ l l l}
    \toprule
    Hyper-Parameter & N-gram Type: Words & N-gram Type: Characters \\
    \midrule
    N-gram Range (Range) & [1,1], [1,2], [1,3], [2,3], [3,3] & [1,5], [1,10], [5,10], [5,15] \\
    Stop Words & English, None & --- \\
    Vector Size (Size) & 10, 100, 300 & 10, 100, 300 \\
    IDF & Yes, No & Yes, No \\
    Norm & L1, L2, None & L1, L2, None \\
    Sub-linear TF & Yes, No & Yes, No \\
    \bottomrule
  \end{tabular}
  \caption{Parameter search space for word and character level N-gram models}
\label{tab:N-gram Hyper-parameters Space}
\end{center}
\end{table}

A grid search was performed to test all combinations of configurations within this hyper-parameter space. Each configuration was evaluated with regards to its \gls{MCC} score using 5-fold cross-validation on the training data with three standard classifiers: Logistic Regression and Naive Bayes and SVM.
Table~\ref{tab:Ngram Grid Search} shows the five best results of the exhaustive grid search over the hyper-parameter configurations.

\begin{table}[h]
  \begin{center}
  \begin{tabular}{ l l l l l l l l }
    \toprule
    Type & Range & Stop words & Size & IDF & Norm & Sub-linear TF & \gls{MCC} \\
    \midrule
    Word & [1,1] & None & 300 & Yes &  & Yes & 0.689 \\
    Word & [1,1] & None & 300 & Yes &  & No & 0.687 \\
    Word & [1,1] & None & 300 & No &  & Yes & 0.682 \\
    Word & [1,1] & None & 300 & No &  & No & 0.682 \\
    Word & [1,1] & None & 300 & Yes & L2 & Yes & 0.68 \\
    \midrule
    Word & [1,1] & None & 300 & No & & Yes & 0.659  \\
    Word & [1,1] & None & 300 & No & & No & 0.656 \\
    Word & [1,2] & None & 300 & No & & Yes & 0.655 \\
    Word & [1,2] & None & 300 & No & & No & 0.655 \\
    Word & [1,3] & None & 300 & No & & No & 0.65 \\
    \midrule
    Word & [1,1] & None & 300 & Yes & & Yes & 0.689 \\
    Word & [1,1] & None & 300 & Yes & & No  & 0.689 \\
    Word & [1,2] & None & 300 & Yes & & Yes & 0.677 \\
    Word & [1,2] & None & 300 & Yes & & No  & 0.677 \\
    Word & [1,3] & None & 300 & Yes & & Yes & 0.674 \\
    \bottomrule
  \end{tabular}
  \caption{Top 5 results of grid search over hyper-parameter space using 5-fold cross-validation on the training set with Logistic Regression (top), Naive Bayes (middle) and SVM (bottom).}
\label{tab:Ngram Grid Search}
\end{center}
\end{table}

The effects of the different hyper-parameters on performance can be observed based on these results. Regarding the N-gram \emph{type}, words as the atomic unit for N-grams consistently led to better results. This is due to the fact that the search space of combinations of characters is significantly larger than the search space of known words, so model complexity increases exponentially.
With regards to the \emph{range}, i.e.\ the interval for possible $N$ in N-grams, there are slight differences to be observed between the three classifiers used, but with all three models the best performance is achieved using Unigrams. Also all of the top results across all classifiers include Unigrams in the model while some extend the range towards bigrams or trigrams.
None of the top results of the performed grid searches used \emph{stop words}. This is interesting as using stop-words to remove hand-picked, highly frequent words that do not carry much meaning is common practice.
When it comes to the \emph{size} of vectors, for the given settings the highest vector dimensionality of 300 achieves the best performance.
There is no consensus between the classifiers on whether or not to weigh the N-gram frequencies by the inverse document frequency (\emph{IDF}, see Section~\ref{par:TF.IDF weighting}). With respect to the \emph{norm} used, in these experiments normalizing vectors decreased performance. Only in the fifth best performing configuration when using Logistic Regression vectors were normalized, in this case using the L2 norm.
Lastly applying sub-linear term frequency scaling (\emph{Sub-linear TF}, see Section~\ref{par:Sublinear TF scaling}) did not seem to affect the results significantly and about half of the top results were obtained using this technique.

Based on these observations the best model for each classifier with regards to \gls{MCC} validation score was chosen and trained on the whole training data and tested on the test data set to estimate the final performance. Table~\ref{tab:Ngram Grid Search Scores} shows the scores of each classifier using these best N-gram models. It is evident that here logistic regression performs best, achieving the highest \gls{MCC} score as well as good accuracy.

\begin{table}
  \begin{center}
  \begin{tabular}{ r | *2l | *2l }
    \toprule
     & \multicolumn{2}{c|}{Training} & \multicolumn{2}{c}{Test}\\
    Classifier & Accuracy & MCC & Accuracy & MCC \\
    \midrule
    Logistic Regression & 0.824 & 0.761 & 0.787 & 0.708 \\
    Naive Bayes         & 0.769 & 0.681 & 0.767 & 0.677 \\
    SVM                 & 0.835 & 0.681 & 0.786 & 0.700 \\
    \bottomrule
  \end{tabular}
  \caption{Performance of each best N-gram model with Logistic Regression and Naive Bayes on the test data.}
\label{tab:Ngram Grid Search Scores}
\end{center}
\end{table}

\begin{figure}
    \centering
    \begin{subfigure}[b]{0.3595\textwidth}
        \includegraphics[width=\textwidth]{img/exp-vector-space/ngram-conf-matrix-logreg-normalized.pdf}
        \caption{Logistic Regression}
\label{fig:ngram-conf-matrix-logreg-normalized}
    \end{subfigure}
    \begin{subfigure}[b]{0.267\textwidth}
        \includegraphics[width=\textwidth]{img/exp-vector-space/ngram-conf-matrix-naivebayes-normalized.pdf}
        \caption{Naive Bayes}
\label{fig:ngram-conf-matrix-naivebayes-normalized}
    \end{subfigure}
    \begin{subfigure}[b]{0.35\textwidth}
        \includegraphics[width=\textwidth]{img/exp-vector-space/ngram-conf-matrix-svm-normalized.pdf}
        \caption{SVM}
\label{fig:ngram-conf-matrix-svm-normalized}
    \end{subfigure}
    \caption{Normalized confusion matrices all three classifiers using the best N-gram model found via cross-validated grid search. Both Naive Bayes as well as SVM show label bias towards the prevalent class \emph{candidate}.}
\label{fig:ngram-conf-matrix}
\end{figure}

To understand the mapping of the data in the resulting vector space visualizations were produced using \gls{PCA} and \gls{t-SNE}. The results for best-performing N-gram model can be seen in Figure~\ref{fig:ngram pca and tsne}. Especially the \gls{PCA} visualization shows that a high percentage of the data for each class can be separated from other classes even with a linear model.

\begin{figure}[h]
    \centering
    \begin{subfigure}[b]{0.48\textwidth}
      \includegraphics[width=\textwidth]{img/exp-vector-space/ngram-pca.pdf}
      \caption{PCA projection}
\label{fig:ngram-pca}
    \end{subfigure}
~
    %add desired spacing between images, e. g. ~, \quad, \qquad, \hfill etc.
    %(or a blank line to force the subfigure onto a new line)
    \begin{subfigure}[b]{0.48\textwidth}
      \includegraphics[width=\textwidth]{img/exp-vector-space/ngram-tsne.pdf}
      \caption{t-SNE projection}
\label{fig:ngram-tsne}
    \end{subfigure}
    \caption{Document vectors produced by the best N-gram model (optimized w.r.t. Logistic Regression) projected onto the first 2 principal components (left) and project using t-SNE projection.}
\label{fig:ngram pca and tsne}
\end{figure}

\subsubsection{Bag-of-Means: An Averaged Word2Vec Model}
\label{subs:Bag-of-Means: An Averaged Word2Vec Model}

Next a Bag-of-Means model as described in Section~\ref{subp:Bag-of-Means} was evaluated with the same set of classifiers. The model was evaluated on the same test and training data split as used for the N-gram model above. The results are shown in Table~\ref{tab:Bag-Of-Means Results}.

\begin{table}[h]
  \begin{center}
  \begin{tabular}{ r | *2l | *2l }
    \toprule
     & \multicolumn{2}{c|}{Training} & \multicolumn{2}{c}{Test}\\
    Classifier & Accuracy & MCC & Accuracy & MCC \\
    \midrule
    Logistic Regression & 0.797 & 0.722 & 0.784 & 0.702 \\
    Naive Bayes         & 0.337 & 0.271 & 0.320 & 0.251 \\
    SVM                 & 0.545 & 0.356 & 0.562 & 0.379 \\
    \bottomrule
  \end{tabular}
  \caption{Performance base classifiers using the Bag-of-Means model}
\label{tab:Bag-Of-Means Results}
\end{center}
\end{table}

As a basis, pre-trained word vectors from the Google News dataset\footnote{The dataset contains contains 300-dimensional vectors for 3 million words and phrases. The phrases were obtained using a simple data-driven approach described in~\cite{Mikolov:2013ab}. The dataset can be obtained on the following website: \url{https://code.google.com/archive/p/word2vec/}} were extracted for the words that occur in the dataset.
Then for each sentence in the data a sentence vector was obtained by taking the arithmetic mean over the vectors for all words in the sentence. Labels were again predicted using Logistic Regression, Naive Bayes and SVM.
We can see that the model performs well using Logistic Regression and it is almost on par with the best N-gram model. On the other hand the variance in results between the classifiers is drastic, and Naive Bayes' score is 0.451 lower that Logistic Regression in absolute terms. The confusion matrices in Figure~\ref{fig:bom-conf-matrix} reveal strong label bias in the case of Naive Bayes and SVM.

\begin{figure}[h]
    \centering
    \begin{subfigure}[b]{0.365\textwidth}
        \includegraphics[width=\textwidth]{img/exp-vector-space/bom-conf-matrix-logreg-normalized.pdf}
        \caption{Logistic Regression}
\label{fig:bom-conf-matrix-logreg-normalized}
    \end{subfigure}
    \begin{subfigure}[b]{0.27\textwidth}
        \includegraphics[width=\textwidth]{img/exp-vector-space/bom-conf-matrix-naivebayes-normalized.pdf}
        \caption{Naive Bayes}
\label{fig:bom-conf-matrix-naivebayes-normalized}
    \end{subfigure}
    \begin{subfigure}[b]{0.345\textwidth}
        \includegraphics[width=\textwidth]{img/exp-vector-space/bom-conf-matrix-svm-normalized.pdf}
        \caption{SVM}
\label{fig:bom-conf-matrix-svm-normalized}
    \end{subfigure}
    \caption{Normalized confusion matrices of all three classifiers using the Bag-of-Means model.}
\label{fig:bom-conf-matrix}
\end{figure}

The visualization in Figure~\ref{fig:bom} shows a projection of the vectors into a 2 dimensional space. We can visually confirm that labels tend to cluster in this space.

\begin{figure}[h!]
    \centering
    \begin{subfigure}[b]{0.48\textwidth}
      \includegraphics[width=\textwidth]{img/exp-vector-space/bom-pca.pdf}
      \caption{PCA projection}
\label{fig:bom-pca}
    \end{subfigure}
~
    %add desired spacing between images, e. g. ~, \quad, \qquad, \hfill etc.
    %(or a blank line to force the subfigure onto a new line)
    \begin{subfigure}[b]{0.48\textwidth}
      \includegraphics[width=\textwidth]{img/exp-vector-space/bom-tsne.pdf}
      \caption{t-SNE projection}
\label{fig:bom-tsne}
    \end{subfigure}
    \caption{Document vectors produced by Bag-of-Means model (optimized w.r.t. Logistic Regression) projected onto the first 2 principal components (left) and projected using t-SNE projection. }
\label{fig:bom}
\end{figure}

\subsubsection{Paragraph Vectors using Distributed Representations}
\label{subs:Paragraph Vectors using Distributed Representations}

Next a vector space model was build using the approach that was proposed by \cite{Le:2014aa} and described in detail in Section~\ref{subp:Paragraph Vectors}. There are several variants and hyper-parameters to this model and their effect was experimentally evaluated. As this model is computationally quite expensive a grid search as for the N-gram model above was infeasible. Hence performance effects were measured by varying the hyper-parameters one at a time while keeping the others fixed. In this process Logistic Regression was used with 5-fold cross-validation. The default configuration of hyper-parameters can be seen in Table~\ref{tab:Paragraph Vector Defaults} below.

\begin{table}[h]
  \begin{center}
  \begin{tabular}{ l | l }
    \toprule
    Vector Size & 100 \\
    Sub-sampling threshold & No \\
    Hierarchical Softmax & Yes \\
    Negative Sampling Value & 3 \\
    Window Size & 10 \\
    Model Type & PV-DBOW \\
    Training Type & Inferred Vectors \\
    \bottomrule
  \end{tabular}
  \caption{Default hyper-parameter configuration for Paragraph Vectors}
  \label{tab:Paragraph Vector Defaults}
\end{center}
\end{table}

\begin{table}[h]
  \begin{center}
    \begin{tabular}{ c | c | c | c }
      \toprule
      Hyper-Parameter & Setting & Training Score & Test Score \\
      \midrule
      \multirow{4}{*}{Vector Size}
       & 2 & 0.229 & 0.223 \\
       & 10  & 0.545 & 0.541 \\
       & 100 & 0.614 & 0.589 \\
       & 300 & 0.648 & \textbf{0.608} \\
      \midrule
      \multirow{4}{*}{Sub-sampling Threshold}
       & None & 0.611 & \textbf{0.588} \\
       & 1e-4 & 0.495 & 0.475 \\
       & 1e-5 & 0.328 & 0.303 \\
       & 1e-6 & 0.149 & 0.127 \\
      \midrule
      \multirow{2}{*}{Hierarchical Softmax}
       & Not used & 0.600 & 0.578 \\
       & Used     & 0.613 & \textbf{0.586} \\
      \midrule
      \multirow{4}{*}{Negative Sampling Value}
       & 0 & 0.598 & 0.575 \\
       & 2 & 0.612 & \textbf{0.591} \\
       & 4 & 0.612 & 0.587 \\
       & 6 & 0.613 & 0.590 \\
      \midrule
      \multirow{3}{*}{Window Size}
       & 5  & 0.611 & 0.586 \\
       & 10 & 0.614 & \textbf{0.588} \\
       & 15 & 0.612 & 0.586 \\
      \midrule
      \multirow{2}{*}{Model Type}
       & PV-DBOW & 0.610 & \textbf{0.580} \\
       & PV-DM   & 0.405 & 0.411 \\
      \midrule
      \multirow{2}{*}{Training Type}
       & Trained Vectors  & 0.519 & 0.366 \\
       & Inferred Vectors & 0.404 & \textbf{0.408} \\
      \bottomrule
    \end{tabular}
  \caption{Test and Training scores measured using \gls{MCC} with the different hyper-parameter settings. In all configurations only one hyper-parameter was adjusted while keeping the others as shown in Table~\ref{tab:Paragraph Vector Defaults}}
\label{tab:Paragraph Vector Parameter Hyper-Parameter Results}
\end{center}
\end{table}

The results of the evaluation experiments of the hyper-parameters can be seen in Table~\ref{tab:Paragraph Vector Parameter Hyper-Parameter Results}. Similarly to the N-Gram models the \emph{vector size} correlates positively with the performance. Again the highest chosen dimensionality was 300 which yielded the highest test score of 0.608. However the difference to a 100-dimensional model is marginal with 2\% absolute loss in performance and a 10-dimensional representation only leads to an absolute loss of 6\% in performance. Further choosing the representation in only two dimensions yields a test score of 0.223. In comparison the best N-gram model achieved an \gls{MCC} score of 0.151 when limited to two dimensions.

With regards to frequent word \emph{sub-sampling} performance was best without its use. Note that this is in contrast with previous work on word vectors where this setting increased performance (see~\cite{Mikolov:2013ab}). In contrast when training the model using the PV-DM architecture instead, frequent word sub-sampling increased the training score, though in this mode the test score was significantly lower as we will see below.  Using \emph{hierarchical softmax} increased the performance, although only leading to a marginal improvement of around 1\% in terms of MCC score. \emph{Negative sampling} generally increased performance. However no trend is apparent as the best performing configurations were a sampling value of 2 and 6 while choosing a value of 4 lead to a slightly lower score. The highest score amongst the tested settings for the window size was achieved with a value of 10 while both a window of 5 and a window of 15 words achieved a lower score.

Both \emph{model types} for paragraph vectors proposed in~\cite{Le:2014aa} were tested, namely the Distributed Bag of Words Model of Paragraph Vectors (PV-DBOW) and the Distributed Memory Model of Paragraph Vectors (PV-DM). The choice of the model type among has a significant influence on the result. As we can see in Table~\ref{tab:Paragraph Vector Parameter Hyper-Parameter Results} the \gls{MCC} test score of the PV-DBOW model is 16.9\% higher in absolute terms.

The choice of vectors to train the classifier, i.e.\ the \emph{training type}, can be seen to have a strong impact on the results as well. When using directly the vectors that are trained with the Paragraph Vector model, performance is notably lower than when inferring new vectors for the training data using the inference procedure described in~\cite{Le:2014aa}. Furthermore when using the model vectors for training the model easily overfits. This effect can be seen in Figure~\ref{fig:doc2vec_training} where the training and test scores are shown over 140 iterations of training. While using the model vectors leads to higher training scores the test scores are significantly lower than when inferring vectors for training.

\begin{figure}[h]
    \centering
    \begin{subfigure}[b]{0.49\textwidth}
      \includegraphics[width=\textwidth]{img/exp-vector-space/doc2vec_training_trained}
      \caption{Training using model vectors}
\label{fig:doc2vec_training_trained}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\textwidth}
      \includegraphics[width=\textwidth]{img/exp-vector-space/doc2vec_training_inferred}
    \caption{Training using inferred vectors}
\label{fig:doc2vec_training_inferred}
    \end{subfigure}
\caption{Comparison of performance when trained the classifier on the trained vectors of the Paragraph Vector model or when inferring new Paragraph Vector vectors for the training data using the model.}
\label{fig:doc2vec_training}
\end{figure}

Lastly the best configurations of hyper-parameters of these approaches were compared in another experiment. Table~\ref{tab:Paragraph Vector Best Configurations Search} lists the tested settings.

\begin{table}[h]
  \begin{center}
  \begin{tabular}{ l | l }
    \toprule
    Vector Size & 300 \\
    Sub-sampling threshold & No \\
    Hierarchical Softmax & \{ No, Yes \} \\
    Negative Sampling Value & \{ 2, 6, 10 \} \\
    Window Size & \{ 8, 10 \} \\
    Model Type & PV-DBOW \\
    Inference Type & Inferred Vectors \\
    \bottomrule
  \end{tabular}
  \caption{Tested Hyper-parameter configurations to find the best Paragraph Vectors model}
  \label{tab:Paragraph Vector Best Configurations Search}
\end{center}
\end{table}

While a \gls{MCC} test score of 0.574 the lowest amongst the candidates the highest score achieved was 0.618. This setting used a window size of 8, a negative sampling value of 10 and hierarchical softmax.

\clearpage

\subsubsection{Evaluation of Classification Methods}
\label{subs:Evaluation of Classification Methods}

\paragraph{Experimental Setup}
\label{par:Experimental Setup}

From the previous experiments on vector space models the best performing setting of each type was chosen, specifically:

\begin{itemize}
  \item \textbf{N-Grams}: Unigrams were used with a dimensionality of 300. No stop words filtering and no vector normalization were applied and while using sub-linear TF and IDF.
  \item \textbf{Bag-Of-Means}: As in the experiments above the approach was simply used with an unweighed arithmetic mean of the word vectors in each sentence.
  \item \textbf{Paragraph Vectors}: PV-DBOW word vectors were generated using inference on the training data, a window size of 8, a negative sampling value of 10, hierarchical softmax and no frequent word sub-sampling.
\end{itemize}

Next a set of established classification methods representative of the algorithm classes described in Section~\ref{sub:Methods For Classification With Vector Space Models} were applied to the data and evaluated.
\todo{show parameter settings of classifiers in appendix}

The dataset was then split into 5 folds to perform \gls{Cross-Validation}. For the results to be reproducible, comparable and to save computational resources, the vector representations of the three types were precomputed for all folds. The full results can be seen in Table~\ref{tab:Classifier Results} while Table~\ref{tab:Test Scores sorted} shows the classifiers' best results sorted by their test score:


\begin{table}[h]
\centering
\begin{tabular}{ l l c }
  \toprule
  Classifier & Vector Space & Test Score \\
  \midrule
  Neural Network & Bag-of-Means & 0.716 \\
  Deep Neural Network & Bag-of-Means & 0.715 \\
  SVM & N-Grams & 0.713 \\
  Logistic Regression & Bag-of-Means & 0.706 \\
  Random Forest & N-Grams & 0.701 \\
  kNN & Bag-of-Means & 0.685 \\
  Naive Bayes & N-Grams & 0.648 \\
  Decision Tree & N-Grams & 0.625 \\
  \bottomrule
\end{tabular}
\caption{The best results in terms of test score for all classifiers.}
\label{tab:Test Scores sorted}
\end{table}

\paragraph{Test Performance}
\label{par:Test Performance}

In terms of test performance, the most important measure when considering the effectiveness of a classification method, the best performing candidate is a single hidden layer Neural Network. On the other hand we can clearly see that there is only a small margin between the top scoring methods and the multi-layer Deep Neural Network as well as SVM's perform virtually at the same level. Also Logistic Regression and Random Forests show comparable performance while kNN, Naive Bayes and Decision Trees can be seen to have a larger gap.


\begin{table}[h]
  \begin{center}
    \begin{tabular}{ ll cc }
      \toprule
      Classifier & Vector Space Model & Training Score & Test Score \\
      \midrule
      \multirow{3}{*}{Logistic Regression}
       & N-Grams & 0.770 & 0.697 \\
       & Bag-of-Means & 0.743 & \textbf{0.706} \\
       & Paragraph Vectors & 0.709 & 0.643 \\
      \midrule
      \multirow{3}{*}{Decision Tree}
       & N-Grams & 0.924 & \textbf{0.625} \\
       & Bag-of-Means & 0.938 & 0.455 \\
       & Paragraph Vectors & 0.994 & 0.290 \\
      \midrule
      \multirow{3}{*}{Naive Bayes}
       & N-Grams & 0.668 & \textbf{0.648} \\
       & Bag-of-Means & 0.498 & 0.491 \\
       & Paragraph Vectors & 0.603 & 0.573 \\
      \midrule
      \multirow{3}{*}{SVM}
       & N-Grams & 0.882 & \textbf{0.713} \\
       & Bag-of-Means & --- & --- \\
       & Paragraph Vectors & 0.831 & 0.683 \\
      \midrule
      \multirow{3}{*}{kNN}
       & N-Grams & 0.929 & 0.642 \\
       & Bag-of-Means & 0.944 & \textbf{0.685} \\
       & Paragraph Vectors & 0.993 & 0.579 \\
      \midrule
      \multirow{3}{*}{Random Forest}
       & N-Grams & 0.924 & \textbf{0.701} \\
       & Bag-of-Means & 0.940 & 0.618 \\
       & Paragraph Vectors & 0.985 & 0.509 \\
      \midrule
      \multirow{3}{*}{Neural Network}
       & N-Grams & 0.911 & 0.709 \\
       & Bag-of-Means & 0.930 & \textbf{0.716} \\
       & Paragraph Vectors & 0.975 & 0.665 \\
      \midrule
      \multirow{3}{*}{Deep Neural Network}
       & N-Grams & 0.913 & 0.708 \\
       & Bag-of-Means & 0.936 & \textbf{0.715} \\
       & Paragraph Vectors & 0.989 & 0.679 \\
      % \midrule
      % \multirow{3}{*}{Convolutional \gls{NN}}
      % & \multirow{3}{*}{---}
      % & N-Grams & --- & --- \\
      % & & Bag-of-Means & --- & --- \\
      % & & Paragraph Vectors & --- & --- \\
      \bottomrule
    \end{tabular}
  \caption{Test and Training performance in terms of \gls{MCC} for the whole set of classifiers. Each classifier was applied to transformed data using the best configuration for each of the three vector space model types. Note that the SVM classifier could not be evaluated in terms of \gls{MCC} score in combination with the Bag-Of-Means vector space model due to unknown reasons.}
\label{tab:Classifier Results}
\end{center}
\end{table}

Regarding vector space models we can see that the Paragraph Vectors did not achieve highest test performance in combination with any of the classifiers. Four of seven classifiers performed best with the Bag-of-Means model and also make up the highest two results while the N-Gram model worked best in the other cases.
Table~\ref{tab:Classifier Results} shows that a set of methods achieve remarkable performance with simple N-Grams. Among them the SVM ranks highest with a \gls{MCC} test score of 0.713. Conversely in many cases the training score using Paragraph Vectors is highest, including both the Tree-based methods, Neural Network approaches and \gls{kNN}.
Another important observation is that the Bag-of-Means model performs best overall and this is consistent half of the classifiers. The highest test score of 0.716 was achieved by a Neural Network using this model.


\begin{wraptable}{r}{0.4\textwidth}
  \centering
\begin{tabular}{ r l }
  \toprule
  Method & Ratio \\
  \midrule
  Naive Bayes & 0.970 \\
  Logistic Regression & 0.950 \\
  SVM & 0.808 \\
  Neural Network & 0.770 \\
  Deep Neural Network & 0.764 \\
  Random Forest & 0.759 \\
  kNN & 0.726 \\
  Decision Tree & 0.676 \\
  \midrule
  N-Grams & 0.795 \\
  Bag-Of-Means & 0.763 \\
  Paragraph Vectors & 0.679 \\
  \bottomrule
\end{tabular}
\caption{Test to training score ratio for classifiers (top) using the best performing vector space model and vector space models (bottom) averaged over classifiers.}
\label{tab:Test/Training Ratio}
\end{wraptable}

\paragraph{Generalization Performance}
\label{par:Generalization Performance}

Another property of classification techniques is their capability to generalize. An indicator for this ability is the ratio between performance on test and training data, since a classifier that simply ``memorizes'' training examples might achieve a very high training score but overfits and fails to classify new data well. Table~\ref{tab:Test/Training Ratio} shows a comparison in terms of this ratio between both the classification algorithms as well as the vector space models when averaged over the performance with all algorithms.

We can see that Naive Bayes and Logistic Regression achieve a very high ratio signaling good generalization.
For the Neural Networks models tested it is apparent that the deep architecture overfits more than the network with a single hidden layer, i.e.\ it achieves higher training but lower test scores. When comparing the Deep Neural Network to Random Forests using Paragraph Vectors though we can see that while both methods achieve similarly high scores on the training data, Random Forests perform significantly worse on the test data --- Random Forests achieve scores of 0.985 (training) and 0.509 (test) as opposed to the scores of 0.989 (training) and 0.679 (test) for the Deep Neural Network.

Naive Bayes behaves very differently with each vector space model. When using N-Grams the model shows decent performance but the score drops with the other models, showing a decrease of performance of 7.5\% with Paragraph Vectors and 15.7\% with Bag-of-Means.

When looking at the vector space models Paragraph Vectors show a test to training score ration which is significantly lower than both other representations with by a margin of 10\% in absolute terms. This means that on average the classifiers tend to overfit on this representation. N-grams on the other hand achieve the highest ration in these terms.


\paragraph{Runtime Complexity}
\label{par:Runtime Complexity}

We can see quite significant variations between the computational requirements for the different methods as Table~\ref{Computation time} shows. Computation of the N-gram model with a dataset of this size (9948 sentences) can be performed in under one second for all five folds. In comparison training the Paragraph Vector model took around 5,5 hours on the same machine. Note that regarding the Bag-Of-Means model the results are not directly comparable: Although computation of this model was fast, this does not account for the fact that it uses pre-computed word embeddings whose computation is almost as complex as training the Paragraph Vector model and was done with a dataset of a billion words.

\begin{table}[h]
  \begin{center}
    \begin{tabular}{ l c c c }
      \toprule
      Algorithm & N-Grams & Bag-Of-Means & Paragraph Vectors \\
      \midrule
      Data Transformation & 0.68s & 1.49m & 5.31h \\
      \midrule
      Logistic Regression & 32.08s & 2.15m & 6.26m \\
      Decision Tree & 30.05s & 4.47m & 4.85m \\
      Naive Bayes & 1.33s & 12.29s & 12.07s \\
      SVM & 15.53m & 1.30h & 1.60h \\
      kNN & 4.24s & 1.16h & 1.23h \\
      Random Forest & 14.82m & 4.24h & 5.65h \\
      \midrule
      Neural Network & 21.32m & 21.26m & 21.04m \\
      Deep Neural Network & 32.84m & 32.65m & 32.22m \\
      \bottomrule
    \end{tabular}
  \caption{Computation time for the experiments. The top row shows the time for generating the feature space models and the rest of the results show the runtime for the different classifiers. The neural network models are separated because they were trained on an \gls{AWS} EC2 \emph{g2.2xlarge} \gls{GPU} instance while the other models were trained on \emph{c4.large} instance using a multi-core CPU. The runtime of all classifiers includedes 5-fold \gls{Cross-Validation} and grid-search over hyper-parameters for most models. Thus the results are only representative to a certain extent as some models had a smaller hyper-parameter space to be evaluated.}
\label{tab:Computation time}
\end{center}
\end{table}

The difference in complexity involved in training the models can also be seen when comparing the computation time of the classifiers with the different vector space models. Here we can see that training and evaluation on 5 folds can be done with an N-Gram model in minutes even with computationally more resource-intensive methods like a Support Vector Machine. Compared to that the runtime of evaluating the Bag-of-Means models takes significantly longer with a varying degree depending on the classifier and this trend potentiates with Paragraph Vectors.

The classifiers with the shortest computation time are the Naive Bayes which can be computed on an N-Gram model in only 1.33 seconds, totaling to around 2 seconds including the construction of the N-Gram vector space, and kNN. Similarly Logistic Regression and Decision Trees can be evaluated in half a minute using N-Grams and the SVM and Random Forest require much more time to compute.

These relations between the classifiers translate to the other vector space models with a few exceptions: The runtime of kNN in the Bag-of-Means and Paragraph Vector spaces becomes much more resource intensive, almost reaching the level needed to compute the SVM model. Naive Bayes keeps computation within a couple of seconds for all models, but at the same time we observe its test score performance decrease with the Bag-of-Means and Paragraph Vector models as mentioned previously.

Both Neural Network models were trained on a GPU instance and as such their computation time shows a very different profile. Despite parallelization of training over GPU cores these methods still have a higher demand on computation time than all other models that were trained on the single core machine when looking at the N-Gram model. On the other hand the time to evaluate these models does not increase with the Bag-of-Means or the Paragraph Vector models as it does for all other methods. In fact we can see a minimal decrease in computation time.


\subsection{Classification With Sequential Models}
\label{sub:Classification With Sequential Models}

\todo{less time spent on optimizing these}

\paragraph{Experimental setup and model choices}
\label{par:Experimental setup and model choices}

Three sequential models were evaluated, each of them based on Long Short-Term Memory Networks (see Section~\ref{subs:Long Short-Term Memory (LSTM) Networks}). All Networks are trained at character level, so instead of indices for words they receive the sentences as a sequence of characters as their input:

\todo{mention embedding layers}

\begin{itemize}
\item \emph{Simple LSTM}: This model uses a single LSTM layer of dimensionality 256. The time steps of this model are 200, meaning that from each sentences the first 200 letters are used and shorter sequences are appended with padding characters. Then a dropout layer is applied, i.e. a layer which prevents forward propagation of each neuron with a given probability as proposed by~\cite{Srivastava:2014aa} to prevent overfitting. Here a probability of 0.5 was used. The output layer consists of six nodes which encode the labels used for classification and uses a softmax activation function.
\item \emph{Stacked LSTM}: The stacked model is similar to the simple model but instead uses three LSTM layers of dimensionality 256, 64 and 32 respectively. The first two layers again return a sequence of the sequence size 200, making the model theoretically capable to learn several levels of abstractions. The last LSTM layer is again connected to the output layer encoding the prediction of labels. Dropout is applied to each LSTM layer with a probability of 0.5.
\item \emph{Multi-task LSTM}: The multi-task model is slightly more complex in its setup. It is in principle a generative model that, given a sequence, produces both the next character and the label that is most likely at this point in the sequence. Like the other models it reads sequences, in this case limited to 40 characters. On top it uses two stacked LSTM layers of which the first one returns sequences itself. The layers have a dimensionality of 512 and dropout with a probability of 0.2 is used. The output layer in this is of size $|K| + |L|$ where $|K|$ is the amount of possible characters and $|L|$ denotes the number or labels. At inference time model was used to step through a sentence one character at a time and for each character the label prediction was used so that the whole sentence would be labelled at character level. Then a majority vote over the labels was performed to retrieve the final prediction of the label for the given sentence.
\end{itemize}

Table~\ref{tab:LSTM Results} shows the results of each model. During training the performance was measured in terms of categorical cross-entropy as the evaluation of \gls{MCC} was computationally quite expensive at each iteration due to a lack of a distributed implementation so that for evaluation a switch between GPU and CPU would have had to be performed. In the table these results are therefor called Training Loss and Test Loss. At test time the score was measured using \gls{MCC}.

\begin{table}[h]
  \begin{center}
    \begin{tabular}{ l c c c }
      \toprule
      Model & Training Loss & Test Loss & Test Score \\
      \midrule
      Simple LSTM & 0.733 & 0.767 & 0.689 \\
      Stacked LSTM & 0.674 & 0.791 & \textbf{0.707} \\
      Multi-task LSTM & --- & --- & 0.258 \\
      \bottomrule
    \end{tabular}
  \caption{Test and training scores for the LSTM recurrent neural network models. Due to limited computational resources the \gls{MCC} score was only evaluated at test time while measuring the categorical cross-entropy loss during training and also at test time.}
\label{tab:LSTM Results}
\end{center}
\end{table}

\begin{wraptable}{r}{0.4\textwidth}
  \centering
  \begin{center}
    \begin{tabular}{ l c c c }
      \toprule
      Model & Runtime \\
      \midrule
      Simple LSTM & 7.04h \\
      Stacked LSTM & 3.05h \\
      Multi-task LSTM & 27.57h \\
      \bottomrule
    \end{tabular}
    \caption{Computation time for the LSTM experiments. These were carried out on a single core instance as opposed to the other Neural Network models above and could be sped up on a \gls{GPU}.}
    \label{tab:Computation time LSTM}
  \end{center}
\end{wraptable}
The first two models perform on par with the classification methods using vector space models discussed in the previously. The stacked model achieves the highest score of 0.707 which is comparable to the Logistic Regression model using a Bag-Of-Means representation but slightly worse than the Neural Network models or the SVM.
The simple LSTM model performs worse with an absolute difference 1.8\% in test performance. The training to test ratios of 0,955 for the Simple LSTM and 0,852 for the stacked LSTM show that the model with one layer tends to overfit much less, despite performing worse overall.
The multi-task LSTM did not nearly achieve similar performance and classifies sentences with an \gls{MCC} test score of 0.258. Training and Test loss in this case were not measured.

Table~\ref{tab:Computation time LSTM} shows the computation time for training and evaluating each of the models. The single LSTM layer network takes more time than the stacked one. It is important to note that these experiments were not carried out on an \gls{AWS} EC2 instance as the experiments for classification with vector space models. Instead they were computed on a personal machine which was in use so the resulting difference in computation is likely to be an artifact of that. The first two models both ran 60 iterations and the best validation loss during training was reached after 30-40 iterations. The computation time for the Multi-task network takes significantly more time with almost 28 hours.

The multi-task network has the nice property that we can sample from it which is a great tool for introspection. Figure~\ref{fig:LSTM multi labeling output} shows three samples of typical output when the network is sequentially processing sentences. For each character a label is produced given the previous characters in the sequence that have already been seen. The brightness of color here encodes the confidence about the label (brighter means higher).

\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{img/LSTM-labeling-output.pdf}
    \caption{Samples of the network sequentially labeling sentences. The brightness of the bluetone behind each character shows the confidence about the prediction, i.e. the probability the networks softmax function outputs it. In these samples the confidence is thus very low (black) or low (dark blue). The labels are: $nextsteps$ ($n$), $job$ ($j$), $benefits$ ($b$), $other$ ($o$) and $candidate$ ($a$ is used as in applicant since $c$ already encodes company).}
\label{fig:LSTM multi labeling output}
\end{figure}

Some interesting observations can be made here, e.g. that the word \emph{work} in the first line immediately triggers the network to output the label \emph{j}. In general though the output fluctuates quite much, and the overall confidence which is achieved by a majority vote over the labels is often just over 50\%.

Another interesting possibility for getting a better understanding of the model's behavior is to let it generate output on its own. The network is primed with sequence of padding characters which contain no information in the dataset as they only extend sequences to the expected length. Then we create a first character and label and this serves as the input for the next prediction. Thus the network is always provided with a truncated history of its own output and generates more on its basis.

Figure~\ref{fig:LSTM multi generating output} shows such output which tells us that the network did not learn much about language. It is unable to produce output that resembles a word and generally stops using other characters than a whitespace after a few characters. Interestingly it still uses different labels at times even when producing whitespace characters.

\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{img/LSTM-generating-output.pdf}
    \caption{Samples of the network generating sequences. At each time step the LSTM is to produce a new character and a new label for this character given the output it has already produced.}
\label{fig:LSTM multi generating output}
\end{figure}
