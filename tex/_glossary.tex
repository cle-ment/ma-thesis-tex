% !TEX root = ../thesis.tex
% !TEX spellcheck = en-US

% A

\newdualentry{API} % label
  {API}            % abbreviation
  {Application Programming Interface}  % long form
  {An Application Programming Interface (API) is a particular set of rules and specifications that a software program can follow to access and make use of the services and resources provided by another particular software program that implements that API.} % description

% \newglossaryentry{Application Programming Interface}{name={Application Programming Interface}, description={An Application Programming Interface (API) is a particular set of rules and specifications that a software program can follow to access and make use of the services and resources provided by another particular software program that implements that API}}
% \newacronym[see={[Glossary:]{Application Programming Interface}}]{API}{API}{Application Programming Interface\glsadd{Application Programming Interface}}

% B

% C

% D

% E

% F

% G

\newglossaryentry{Grid Search}{name={Grid Search}, description={Grid Search describes the systematic exhaustive search over a space of hyper-parameters. Usually this is combined with \gls{Cross-Validation} in order to test each hyper-parameter combination within reasonable bounds to find the best configuration for a given model or algorithm.}}

\newglossaryentry{Gold Standard}{name={Gold Standard}, description={see \gls{Ground Truth}}}

\newglossaryentry{Ground Truth}{name={Ground Truth}, description={The ground truth, also sometimes referred to as \gls{Gold Standard}, is the known set of labels in \gls{Supervised Learning} which forms the target for learning and is thus also used for evaluating a learning algorithm.}}

% H

% I

% J

% K

% L

\newglossaryentry{Lloyds Algorithm}{name={Lloyds Algorithm}, description={Lloyd's algorithm, introduced by~\cite{Lloyd:1982aa}, is an method for the tackling the K-means clustering problem, i.e. to partition an input space by finding cluster centroids representing the mean of each cluster's points so that each point belongs to the cluster whose centroid is closest.}}

\newglossaryentry{Latent Dirichlet Allocation}{name={Latent Dirichlet Allocation}, description={Latent Dirichlet Allocation is a popular algorithm for \gls{Topic Modeling} proposed by~\cite{Blei:2003aa}. Conceptually it assumes that a document is generated first sampling a set of topics from a distribution of topics and then sampling words from each of these topics distributions of words. Learning topics from documents can then be achieved by figuratively inverting this process and inferring topics from the distributions of words under the same assumption how the document was generated.}}
\newacronym[see={[Glossary:]{Latent Dirichlet Allocation}}]{LDA}{LDA}{Latent Dirichlet Allocation}

% M

% N

% O

% P

% Q

% R

\newglossaryentry{Representational state transfer}{name={Representational state transfer (REST)}, description={\textquote{Representational state transfer (REST) is an architectural style used for web development. [\ldots] RESTful systems typically, but not always, communicate over Hypertext Transfer Protocol (HTTP) with the same HTTP verbs (GET, POST, PUT, DELETE, etc.) that web browsers use to retrieve web pages and to send data to remote servers.} Source: \url{https://en.wikipedia.org/wiki/Representational_state_transfer}}}
\newacronym[see={[Glossary:]{Representational state transfer}}]{REST}{REST}{Representational state transfer\glsadd{Representational state transfer}}

% S

% T

\newglossaryentry{Topic Modeling}{name={Topic Modeling}, description={Topic Modeling refers to the research problem of identifying the prevalent topics for a set of documents. This problem is usually in terms of \gls{Unsupervised Learning} since topic \gls{Ground Truth} exists beforehand. A popular algorithm for Topic Modeling is \acrshort{LDA}.}}


% U

% V

% W

% X

% Y

% Z










\newglossaryentry{Supervised Learning}{name={Supervised Learning}, description={TODO}}

\newglossaryentry{Unsupervised Learning}{name={Unsupervised Learning}, description={TODO}}

\newglossaryentry{Amazon Mechanical Turk}{name={Amazon Mechanical Turk}, description={TODO}}

\newglossaryentry{scikit-learn}{name={scikit-learn}, description={TODO}}

\newglossaryentry{mikro-tasking}{name={Mikro-tasking}, description={TODO}}

\newglossaryentry{K-means clustering}{name={K-means clustering}, description={TODO}}

\newglossaryentry{silhouette score}{name={silhouette score}, description={TODO}}

\newacronym[see={[Glossary:]{k Nearest Neighbors}}]{kNN}{kNN}{k Nearest Neighbors\glsadd{k Nearest Neighbors}}


\newglossaryentry{k Nearest Neighbors}{name={$k$-Nearest Neighbors},
    description={The $k$-Nearest Neighbors algorithm is a non-parametric method for classification or regression tasks. See Section~\ref{subs:Instance-based Methods}}}



\newglossaryentry{MCC}{type=\acronymtype, name={MCC}, description={}, first={Matthews correlation coefficient (MCC)\glsadd{Matthews correlation coefficient}}, see=[Glossary:]{MCC}}


\newglossaryentry{Matthews correlation coefficient}{name={Matthews correlation coefficient},
    description={TODO}}

\newglossaryentry{CNN}{type=\acronymtype, name={CNN}, description={Convolutional Neural Network}, plural={Convolutional Neural Networks (CNN's)}, first={Convolutional Neural Network (CNN)\glsadd{Convolutional Neural Network}}, see=[Glossary:]{Convolutional Neural Network}}

\newglossaryentry{RNN}{type=\acronymtype, name={RNN}, description={Recurrent Neural Network}, plural={Recurrent Neural Networks (CNN's)}, first={Recurrent Neural Network (CNN)\glsadd{Recurrent Neural Network}}, see=[Glossary:]{Recurrent Neural Network}}

\newglossaryentry{Recurrent Neural Network}{name={Recurrent Neural Network (CNN)},
description={todo}}

\newglossaryentry{Convolutional Neural Network}{name={Convolutional Neural Network (CNN)},
description={todo}}

\newglossaryentry{NN}{type=\acronymtype, name={NN}, description={Neural Network}, plural={Neural Networks}, first={Neural Network (ANN)\glsadd{Neural Network}}, see=[Glossary:]{Neural Network}}


\newglossaryentry{Neural Network}{name={Neural Networks (ANN)},
description={or Artificial Neural Network (NN), a learning algorithm inspired by the \todo{finish}. See Section~\ref{par:K Nearest Neighbors}}}

\newglossaryentry{GPU}{type=\acronymtype, name={GPU}, description={Graphical Processing Unit}, plural={Graphical Processing Units}, see=[Glossary:]{Graphical Processing Unit}}

\newglossaryentry{Graphical Processing Unit}{name={Graphical Processing Unit (GPU)},
description={TODO}}

\newglossaryentry{CL}{type=\acronymtype, name={CL}, description={Computational Linguistics}, first={Computational Linguistics (CL)}} % glossary entry??

\newglossaryentry{NLP}{type=\acronymtype, name={NLP}, description={Natural Language Processing}, first={Natural Language Processing (NLP)}} % glossary entry??

\newglossaryentry{AI}{type=\acronymtype, name={AI}, description={Artificial Intelligence}, first={Artificial Intelligence (AI)}} % glossary entry??

\newglossaryentry{ML}{type=\acronymtype, name={ML}, description={Machine Learning}, first={Machine Learning (ML)}}

\newglossaryentry{Machine Learning}{name={Machine Learning},
description={TODO}}

\newglossaryentry{Cross-Validation}{name={Cross-Validation},
description={TODO}}

\newglossaryentry{IR}{type=\acronymtype, name={IR}, description={Information Retrieval}, first={Information Retrieval (IR)}} % glossary entry??

\newglossaryentry{TC}{type=\acronymtype, name={TC}, description={Text Classification}, first={Text Classification (TC)}} % glossary entry??

\newglossaryentry{NLTK}{type=\acronymtype, name={NLTK}, description={Natural Language Toolkit}, first={Natural Language Toolkit (NLTK)\glsadd{Natural Language Toolkit}}, see=[Glossary:]{Natural Language Toolkit}}

\newglossaryentry{Natural Language Toolkit}{name={Natural Language Toolkit (NLTK)}, description={``NLTK is a leading platform for building Python programs to work with human language data. It provides easy-to-use interfaces to over 50 corpora and lexical resources such as WordNet, along with a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning and wrappers for industrial-strength NLP libraries.'' See \url{http://www.nltk.org}}}

\newglossaryentry{langdetect}{name={langdetect},
    description={Language detection library ported from Google's language-detection (\url{https://code.google.com/p/language-detection/}). See \url{https://pypi.python.org/pypi/langdetect?}}}

\newglossaryentry{Beautiful Soup}{name={Beautiful Soup},
    description={Beautiful Soup is a Python library designed for quick turnaround projects like screen-scraping. See \url{https://www.crummy.com/software/BeautifulSoup/}}}

\newglossaryentry{confusion matrix}{name={Confusion Matrix},
    description={todo}}

\newglossaryentry{Oikotie Tyopaikat}{name={Oikotie Ty√∂paikat},
    description={todo}}

\newglossaryentry{Node.js}{name={Node.js},
    description={\textquote{Node.js is an open-source, cross-platform runtime environment for developing server-side Web applications.}Source: \url{https://en.wikipedia.org/wiki/Node.js} \url{https://nodejs.org/}}}

\newglossaryentry{MongoDB}{name={MongoDB},
    description={\textquote{MongoDB (from humongous) is a free and open-source cross-platform document-oriented database. [\ldots].} Source: \url{https://en.wikipedia.org/wiki/MongoDB}, Website: \url{https://www.mongodb.com}}}

\newglossaryentry{Representation Learning}{name={Representation Learning},
    description={TODO}}

\newglossaryentry{Mustache}{name={Mustache},
    description={\textquote{Mustache is a simple web template system.} Source: \url{https://en.wikipedia.org/wiki/Mustache_(template_system)}, Website: \url{https://mustache.github.io}}}


\newglossaryentry{Sanoma}{name={Sanoma},
    description={todo}}

\newglossaryentry{error matrix}{name={Error Matrix},
    description={todo},see=[Glossary:]{confusion matrix}}

\newglossaryentry{ffe}{type=\acronymtype, name={FFE}, description={Fuzzy Front End}, first={Fuzzy Front End (FFE)\glsadd{ffeg}}, see=[Glossary:]{ffeg}}

\newglossaryentry{ffeg}{name={Fuzzy Front End},
    description={The early exploration phase in the design process (see~\cite{Council:2007aa}). Also refers to an awesome band from Finland.}}

\newglossaryentry{Ensemble Method}{name={Ensemble Method}, plural={Ensemble Methods}, description={TODO}}

\newglossaryentry{Google Docs}{name={Google Docs}, description={TODO}}

\newglossaryentry{Randomized Algorithm}{name={Randomized Algorithm}, plural={Randomized Algorithms}, description={TODO}}

\newglossaryentry{Boosting}{name={Boosting}, description={TODO}}

\newglossaryentry{Bagging}{name={Bagging}, description={TODO}}

\newglossaryentry{Voting}{name={Voting}, description={TODO}}

\newglossaryentry{Feature Selection}{name={Feature Selection}, description={TODO}}

\newglossaryentry{Feature Extraction}{name={Feature Extraction}, description={TODO}}

\newglossaryentry{click-through rate}{name={Click-through rate}, description={TODO}}

\newglossaryentry{page views}{name={page views}, description={TODO}}

\newglossaryentry{Principal Components Analysis}{name={Principal Components Analysis}, description={TODO}}

\newglossaryentry{PCA}{type=\acronymtype, name={PCA}, description={Principal Components Analysis}, first={Principal Components Analysis (PCA)\glsadd{Principal Components Analysis}}, see=[Glossary:]{Principal Components Analysis}}

\newglossaryentry{t-Distributed Stochastic Neighbor Embedding}{name={t-Distributed Stochastic Neighbor Embedding}, description={t-Distributed Stochastic Neighbor Embedding (t-SNE) is a (prize-winning) technique for dimensionality reduction that is particularly well suited for the visualization of high-dimensional datasets. For more information visit the authors information website (\url{https://lvdmaaten.github.io/tsne/}) and see the introductory publication (\cite{Maaten:2008aa})}}

\newglossaryentry{t-SNE}{type=\acronymtype, name={t-SNE}, description={t-Distributed Stochastic Neighbor}, first={t-Distributed Stochastic Neighbor (t-SNE)\glsadd{t-Distributed Stochastic Neighbor Embedding}}, see=[Glossary:]{t-Distributed Stochastic Neighbor Embedding}}

\newglossaryentry{Dimensionality Reduction}{name={Dimensionality Reduction}, description={TODO}}

\newglossaryentry{Multi-Class Classification}{name={Multi-Class Classification}, description={TODO}}

\newglossaryentry{Multi-Label Classification}{name={Multi-Label Classification}, description={TODO}}

\newglossaryentry{SVM}{type=\acronymtype, name={SVM}, description={Support Vector Machine}, first={Support Vector Machine (SVM)\glsadd{Support Vector Machine}}, see=[Glossary:]{Support Vector Machine}}

\newacronym[see={[Glossary:]{JavaScript Object Notation}}]{JSON}{JSON}{JavaScript Object Notation\glsadd{JavaScript Object Notation}}

% \newglossaryentry{JSON}{type=\acronymtype, name={JSON}, description={JavaScript Object Notation}, first={JavaScript Object Notation (JSON)\glsadd{JavaScript Object Notation}}, see=[Glossary:]{JavaScript Object Notation}}

\newglossaryentry{JavaScript Object Notation}{name={JavaScript Object Notation}, description={\textquote{JSON (JavaScript Object Notation) is a lightweight data-interchange format. It is easy for humans to read and write. It is easy for machines to parse and generate. It is based on a subset of the JavaScript Programming Language, Standard ECMA-262 3rd Edition - December 1999. JSON is a text format that is completely language independent but uses conventions that are familiar to programmers of the C-family of languages, including C, C++, C\#, Java, JavaScript, Perl, Python, and many others. These properties make JSON an ideal data-interchange language.} Source: \url{http://www.json.org} }}


\newglossaryentry{Support Vector Machine}{name={Support Vector Machine}, description={TODO}}

\newglossaryentry{Deep Learning}{name={Deep Learning},
    description={TODO}}

\newglossaryentry{CrowdFlower}{name={CrowdFlower},
    description={TODO}}

\newglossaryentry{crowd sourcing}{name={crowd sourcing},
    description={TODO}}

\newglossaryentry{GitHub}{name={GitHub},
    description={TODO}}

% SVM & Support Vector Machine \\
% NN & Neural Network \\
% RNN & Recurrent Neural Network \\
% LSTM & Long Short-Term Memory \\
% MCC & Matthews Correlation Coefficient, see Section~\ref{par:Informedness, Markedness and Matthews Correlation Coefficient}


% Bayes' Theorem
% one-hot-encoding
% grid search
% Mturk & see \emph{Mechanical Turk} \\

% MongoDB
% Mongoose
% GitHub
% decision boundary
% discriminant function
% gradient descent
% Bias-Variance Dilemma
