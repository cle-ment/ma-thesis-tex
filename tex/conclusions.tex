% !TEX root = ../thesis.tex
% !TEX spellcheck = en-US

\clearpage

\section{Conclusions}
\label{sec:Conclusions}


- while the approaches are presented as two different techniques here the of course are complementary and somewhat overlapping. After all the LSTM uses embeddings which effectively learn vector representations for words (is that true)?
- many of these techniques can be and are mixed and matched, e.g. using word embeddings and modeling sentences in a sequential fashion, possibly even putting a CNN on top to learn invariantly

- it's a vast and fast moving field. just in the last weeks of writing this thesis facebooks fasttext was released https://github.com/facebookresearch/fastText (link to papers). this also shows the importance of this problem

\begin{itemize}
  \item For complex models need more data (NNs)
  \item can do much more parameter tuning for NNs
  \item small documents like sentences are hard to learn as a representation
\end{itemize}

- need more data to train NNs

- good generalization => can be reliable because we know it'll perform well on future unseen data


- lower vector size and overfitting for continous representations: I would have needed more data => could have trained the model with all ads I have and then only do supervised training on labelled data

\begin{itemize}
  \item As in many areas of machine learning much work has been going into feature engineering but it seems that feature learning, while much more computationally expensive, surpasses the potential of engineered feature representations. Deep learning and meta-learning are mature enough to make up for the gap that has been there for years: To achieve performance that is good enough to make an algorithmic system usable in production, huge amounts of research and engineering went into feature engineering and finally the performance of these methods can be matched and even surpassed by automated methods or learning features. (link here NG's transfer learning work, also Schmidhubers work of meta-learning and on function prediction etc)
  \item There is more need to understand the representations of such feature learning systems though, statistics are quite easy to understand but weights of a neural network don't tell much. There is however potential for learning ``better statistics'' ourselves, e.g. how to efficiently learn a language (by looking at explicit indermediate representations of the states of a NN)
  \item
\end{itemize}


% \subsection{Contributions *}
% \label{sub:contributions}
%
% \begin{itemize}
%   \item compare n-gram and doc2vec (?)
%   \item
% \end{itemize}

\subsection*{Proposal for Future Research}
\label{sub:further-research}

Make this a structured prediction task: Closer to the original task of finding the semantic structure of a job ad. Predict a whole job, automatically chop it into categories. The knowledge about the context of the sentence carries strong prior knowledge for deciding a sentence category. Using hierarchical models or priors might make a big difference. Also in the sequential modeling setting this will most probably improve performance significantly.

\begin{itemize}
	\item pretrain doc vectors with the whole corpus (since it can be done unsupervised)
  \item how well do word2vec and comparable methods generalize: e.g.\ initialize a text corpus with word vectors from a bigger corpus (Google News), then train an RNN to predict the next word vector using the small corpus but use the bigger corpus to validate and see if words in bigger corpus can be inferred
  \item trajectory based algorithms (word trajectory through space for a sentence)
  \item Compare with standard benchmarks (TREC etc)
  \item Meta- / Transfer-learning: OCR with simultaneous LM learning (e.g. predict next character)
  \item Try on Finnish data !!!
  \item Try longer parts again maybe with better seperation (paragraphs). Doc2vec gets way better accuracy when documents are longer
  \item Doc2Vec, evaluate how close inferred vectors are to trained vectors
  \item combine sequential approaches with representations gained from external knowledge or trained simultaneously (learning embeddings for semantic concepts)
  \item for complex models like CNNs and RNNs need more data!
  \
\end{itemize}


\paragraph{Trajectory -Based Algorithms on Text}
\label{par:Trajectory Algorithms on Text}
As shown in \cite{Mikolov:2013ac} and related work that was outlined in Section\todo{reference here} vector space models for text can capture very subtle semantic relationships between words by their location in the vector space. When


\subsection*{Learnings}
\label{sub:learnings}

\begin{itemize}
  \item focusing on both, building a working system (engineering) and exploring new directions (science), is hard
  \item problem framing is hard
  \item Learned AWS
  \item building pipelines
  \item deep learning
  \item collecting data
\end{itemize}
