% !TEX root = ../thesis.tex
% !TEX spellcheck = en-US

\clearpage

\section{Conclusions}
\label{sec:Conclusions}

This thesis has applied an explorative and iterative poduct development process to define and then research a machine learning problem and approaches to solve it. The research problem investigated was set to be multi-class text classification on sentence data. Despite being an well-known and simple problem in its nature it is far from being solved and the topic of much recent works, also because of it's wide applicability to various real world problem domains such as sentiment analysis, spam identification and document filtering.

By investigating a wide array of methods on a generated dataset a few main results can be concluded:

Recent work in \gls{CL} and \gls{NLP} shows a trend towards more general purpose methods which avoid built-in assumptions about the problem domain and are therefore applicable through various domains. We saw in this work that beyond potentially freeing us from the often involved and tedious work of \gls{Feature Engineering} these methods also perform on par or even better. This is because they inform their data representation choices in a data-driven way and with enough data and a well chosen model design are shown to increasingly outperform hand-designed feature extraction techniques. 

These trends were greatly accalerated by the recent wave in popularity of Deep Learning methods and more generally connectionist models where ideas from \gls{Representation Learning} and Unsupervised Feature Learning have been a focus of research for decades, by the growth in large data and its

 which were seperated into two conceptual categories


- promise of more data driven models, reducing bias through domain knowledge
- great because it also makes methods applicable across domains
- even in problems that seem simple or solved new developments can make a difference
-


- while the approaches are presented as two different techniques here the of course are complementary and somewhat overlapping. After all the LSTM uses embeddings which effectively learn vector representations for words (is that true)?
- many of these techniques can be and are mixed and matched, e.g. using word embeddings and modeling sentences in a sequential fashion, possibly even putting a CNN on top to learn invariantly

- it's a vast and fast moving field. just in the last weeks of writing this thesis facebooks fasttext was released https://github.com/facebookresearch/fastText (link to papers). this also shows the importance of this problem

\begin{itemize}
  \item For complex models need more data (NNs)
  \item can do much more parameter tuning for NNs
  \item small documents like sentences are hard to learn as a representation
\end{itemize}

- need more data to train NNs

- good generalization => can be reliable because we know it'll perform well on future unseen data


- lower vector size and overfitting for continous representations: I would have needed more data => could have trained the model with all ads I have and then only do supervised training on labelled data

- shows how to apply product development processes to research successfully

\begin{itemize}
  \item As in many areas of machine learning much work has been going into feature engineering but it seems that feature learning, while much more computationally expensive, surpasses the potential of engineered feature representations. Deep learning and meta-learning are mature enough to make up for the gap that has been there for years: To achieve performance that is good enough to make an algorithmic system usable in production, huge amounts of research and engineering went into feature engineering and finally the performance of these methods can be matched and even surpassed by automated methods or learning features. (link here NG's transfer learning work, also Schmidhubers work of meta-learning and on function prediction etc)
  \item There is more need to understand the representations of such feature learning systems though, statistics are quite easy to understand but weights of a neural network don't tell much. There is however potential for learning ``better statistics'' ourselves, e.g. how to efficiently learn a language (by looking at explicit indermediate representations of the states of a NN)
  \item
\end{itemize}


% \subsection{Contributions *}
% \label{sub:contributions}
%
% \begin{itemize}
%   \item compare n-gram and doc2vec (?)
%   \item
% \end{itemize}

\subsection*{Proposal for Future Research}
\label{sub:further-research}

Make this a structured prediction task: Closer to the original task of finding the semantic structure of a job ad. Predict a whole job, automatically chop it into categories. The knowledge about the context of the sentence carries strong prior knowledge for deciding a sentence category. Using hierarchical models or priors might make a big difference. Also in the sequential modeling setting this will most probably improve performance significantly.

\begin{itemize}
	\item pretrain doc vectors with the whole corpus (since it can be done unsupervised)
  \item how well do word2vec and comparable methods generalize: e.g.\ initialize a text corpus with word vectors from a bigger corpus (Google News), then train an RNN to predict the next word vector using the small corpus but use the bigger corpus to validate and see if words in bigger corpus can be inferred
  \item trajectory based algorithms (word trajectory through space for a sentence)
  \item Compare with standard benchmarks (TREC etc)
  \item Meta- / Transfer-learning: OCR with simultaneous LM learning (e.g. predict next character)
  \item Try on Finnish data !!!
  \item Try longer parts again maybe with better seperation (paragraphs). Doc2vec gets way better accuracy when documents are longer
  \item Doc2Vec, evaluate how close inferred vectors are to trained vectors
  \item combine sequential approaches with representations gained from external knowledge or trained simultaneously (learning embeddings for semantic concepts)
  \item for complex models like CNNs and RNNs need more data!
  \
\end{itemize}


\paragraph{Trajectory -Based Algorithms on Text}
\label{par:Trajectory Algorithms on Text}
As shown in \cite{Mikolov:2013ac} and related work that was outlined in Section\todo{reference here} vector space models for text can capture very subtle semantic relationships between words by their location in the vector space. When


\subsection*{Learnings}
\label{sub:learnings}

\begin{itemize}
  \item focusing on both, building a working system (engineering) and exploring new directions (science), is hard
  \item problem framing is hard
  \item Learned AWS
  \item building pipelines
  \item deep learning
  \item collecting data
\end{itemize}
