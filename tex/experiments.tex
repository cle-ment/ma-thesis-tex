% !TEX root = ../thesis.tex
% !TEX spellcheck = en-US

\clearpage
\section{Experiments}

\subsection{TODO}

\begin{itemize}
  \item Comparison one-vs-rest and one-vs-one against linear machine
  \item Visualizations and embeddings of data in 2D (and decision boundaries?)
  \item show T-SNE embeddings of doc2vec vectors
\end{itemize}



\subsection{Classification of paragraph labels}

\subsection{Unsupervised label detection}



\subsection{Classification of sentence labels}

\subsubsection{Experiment 1: Feature Extraction through Vector Space Models}

\subsubsection{N-gram models}
\label{ngram-models}



\subsubsection{Distributed Representations language models}
\label{doc2vec}

Following an approach proposed by \cite{Le:2014aa}, the rationale of this experiment was to find out it was possible to obtain a more expressive language model than the N-gram based methods described in \ref{ngram-models}.

The idea is based

\subsection{Neural Networks}
