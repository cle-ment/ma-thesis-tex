\contentsline {todo}{link section}{3}{section*.9}
\contentsline {todo}{section on transfer learning and feature learning}{4}{section*.10}
\contentsline {todo}{text classification}{4}{section*.11}
\contentsline {todo}{Multitask learning}{4}{section*.12}
\contentsline {todo}{explicit vs implicit feature representation}{4}{section*.13}
\contentsline {todo}{reference}{6}{section*.15}
\contentsline {todo}{example with 2 vectors and showing what they encode?}{7}{section*.16}
\contentsline {todo}{citation for first or review paper here?}{8}{section*.18}
\contentsline {todo}{mention smoothing techniques \cite {Chen:1996aa}}{9}{section*.26}
\contentsline {todo}{citation for Confusion Matrix?}{17}{section*.45}
\contentsline {todo}{more detail?}{19}{section*.50}
\contentsline {todo}{Describe data format}{20}{section*.51}
\contentsline {todo}{Picture of software setup?}{20}{section*.52}
\contentsline {todo}{Describe data: Different characteristics}{21}{section*.54}
\contentsline {todo}{show distribution?}{21}{section*.55}
\contentsline {todo}{show embedding visualizations}{21}{section*.56}
\contentsline {todo}{Comparison one-vs-rest and one-vs-one against linear machine}{21}{section*.59}
\contentsline {todo}{Visualizations and embeddings of data in 2D (and decision boundaries?)}{21}{section*.60}
\contentsline {todo}{show T-SNE embeddings of doc2vec vectors}{21}{section*.61}
\contentsline {todo}{Doc2Vec model is evaluated in 2 ways (normal and trained on inferred vectors)}{26}{section*.64}
\contentsline {todo}{say why using the sentence dataset here}{26}{section*.65}
\contentsline {todo}{reference jupyter notebook here}{26}{section*.66}
\contentsline {todo}{actually discuss time and memory requirements}{26}{section*.67}
\contentsline {todo}{reference section here}{26}{section*.68}
\contentsline {todo}{link to logistic regression classifier explanation here}{26}{section*.69}
\contentsline {todo}{link accuracy?}{26}{section*.70}
\contentsline {todo}{Why are the grid scores lower than the latter scores on the train/test split? Because they're averaged and only on the training data?}{27}{section*.74}
\contentsline {todo}{properly align visualization}{29}{section*.84}
\contentsline {todo}{mention one-vs-all scheme for log reg? also for ngrams above}{31}{section*.87}
\contentsline {todo}{write a bit more here}{35}{section*.104}
\contentsline {todo}{This section in further research? Because it would have to be done for N-grams as well (building a model with the context around a sentence) and it doesn't fit the task (only sentence given). Could also go into exploration}{35}{section*.106}
\contentsline {todo}{if this is described here it has to go into background as well}{36}{section*.107}
\contentsline {todo}{reference here}{38}{section*.109}
